# ğŸ› ï¸ å¼€å‘æŒ‡å—

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### ç³»ç»Ÿæ¦‚è§ˆ

```mermaid
graph TD
    A[input.txt] --> B[Step 0: æ–‡æœ¬åˆ†å‰²]
    B --> C[input.docx]
    C --> D[Step 1: AIåˆ†æ]
    D --> E[txt.xlsx]
    E --> F[Step 2: å›¾åƒç”Ÿæˆ]
    E --> G[Step 3: è¯­éŸ³åˆæˆ]
    F --> H[image/]
    G --> I[voice/]
    H --> J[Step 4: è§†é¢‘åˆæˆ]
    I --> J
    J --> K[video/final.mp4]
```

### æ ¸å¿ƒæ¨¡å—

#### 1. é…ç½®ç®¡ç† (`config.py`)
```python
# ç»Ÿä¸€é…ç½®ç®¡ç†
from config import config

# è®¿é—®é…ç½®
llm_provider = config.llm_provider
api_key = config.openai_api_key
```

**ä¸»è¦åŠŸèƒ½**:
- ç¯å¢ƒå˜é‡åŠ è½½å’ŒéªŒè¯
- é…ç½®ç±»å‹è½¬æ¢å’Œé»˜è®¤å€¼
- è·¯å¾„ç®¡ç†å’Œç›®å½•åˆ›å»º

#### 2. LLMå®¢æˆ·ç«¯ (`llm_client.py`)
```python
# ç»Ÿä¸€LLMæ¥å£
from llm_client import llm_client

# ä½¿ç”¨ç¤ºä¾‹
translation = llm_client.translate_to_english(text)
storyboard = llm_client.translate_to_storyboard(text)
```

**è®¾è®¡ç‰¹ç‚¹**:
- æ”¯æŒå¤šæœåŠ¡å•† (OpenAI, DeepSeek)
- ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†
- é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- ç»Ÿä¸€æ¥å£æŠ½è±¡

#### 3. ä¸šåŠ¡æ¨¡å—

| æ¨¡å— | åŠŸèƒ½ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| `step0` | æ–‡æœ¬åˆ†å‰² | `input.txt` | `input.docx` |
| `step1` | AIåˆ†æç¿»è¯‘ | `input.docx` | `txt.xlsx` |
| `step2` | å›¾åƒç”Ÿæˆ | `txt.xlsx` | `image/*.png` |
| `step3` | è¯­éŸ³åˆæˆ | `txt.xlsx` | `voice/*.wav` |
| `step4` | è§†é¢‘åˆæˆ | `image/`, `voice/` | `video/*.mp4` |

## ğŸ”§ å¼€å‘ç¯å¢ƒè®¾ç½®

### å¼€å‘å·¥å…·æ¨è

```bash
# ä»£ç æ ¼å¼åŒ–
pip install black isort

# ä»£ç æ£€æŸ¥
pip install flake8 mypy

# æµ‹è¯•æ¡†æ¶
pip install pytest pytest-cov
```

### å¼€å‘å·¥ä½œæµ

```bash
# 1. æ¿€æ´»å¼€å‘ç¯å¢ƒ
./activate_env.sh

# 2. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# 3. ä»£ç æ ¼å¼åŒ–
black .
isort .

# 4. ä»£ç æ£€æŸ¥
flake8 .
mypy .

# 5. è¿è¡Œæµ‹è¯•
pytest tests/
```

## ğŸ“ ä»£ç è§„èŒƒ

### Pythonç¼–ç è§„èŒƒ

1. **éµå¾ªPEP 8**
   - ä½¿ç”¨4ç©ºæ ¼ç¼©è¿›
   - è¡Œé•¿åº¦é™åˆ¶100å­—ç¬¦
   - å‡½æ•°å’Œç±»å‘½åä½¿ç”¨snake_caseå’ŒPascalCase

2. **æ–‡æ¡£å­—ç¬¦ä¸²**
   ```python
   def translate_to_english(text: str) -> str:
       """å°†ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºè‹±æ–‡
       
       Args:
           text: éœ€è¦ç¿»è¯‘çš„ä¸­æ–‡æ–‡æœ¬
           
       Returns:
           ç¿»è¯‘åçš„è‹±æ–‡æ–‡æœ¬
           
       Raises:
           APIError: å½“APIè°ƒç”¨å¤±è´¥æ—¶
       """
   ```

3. **ç±»å‹æ³¨è§£**
   ```python
   from typing import List, Dict, Optional
   
   def process_sentences(sentences: List[str]) -> Dict[str, str]:
       return {"result": "processed"}
   ```

### é”™è¯¯å¤„ç†

```python
try:
    result = api_call()
except requests.exceptions.RequestException as e:
    logger.error(f"APIè¯·æ±‚å¤±è´¥: {e}")
    raise APIError(f"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {e}")
except Exception as e:
    logger.exception("æœªçŸ¥é”™è¯¯")
    raise
```

### æ—¥å¿—è®°å½•

```python
import logging

logger = logging.getLogger(__name__)

def process_text(text):
    logger.info(f"å¼€å§‹å¤„ç†æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")
    try:
        result = do_processing(text)
        logger.info("æ–‡æœ¬å¤„ç†å®Œæˆ")
        return result
    except Exception as e:
        logger.error(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
        raise
```

## ğŸ”Œ APIé›†æˆ

### LLMæœåŠ¡é›†æˆ

#### æ·»åŠ æ–°çš„LLMæä¾›å•†

1. **æ‰©å±•é…ç½®**
   ```python
   # config.py
   @property
   def new_provider_api_key(self) -> str:
       return os.getenv('NEW_PROVIDER_API_KEY', '')
   ```

2. **æ‰©å±•å®¢æˆ·ç«¯**
   ```python
   # llm_client.py
   def _setup_client(self):
       if self.provider == 'new_provider':
           self.client = NewProviderClient(
               api_key=config.new_provider_api_key
           )
           self.model = config.new_provider_model
   ```

3. **æ·»åŠ ç¯å¢ƒå˜é‡**
   ```env
   # .env
   LLM_PROVIDER=new_provider
   NEW_PROVIDER_API_KEY=your_key
   NEW_PROVIDER_MODEL=new_model
   ```

### Stable Diffusioné›†æˆ

#### APIè°ƒç”¨ç¤ºä¾‹

```python
import requests

def generate_image(prompt: str, negative_prompt: str = "") -> bytes:
    payload = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "steps": config.sd_steps,
        "cfg_scale": config.sd_cfg_scale,
        "width": config.sd_width,
        "height": config.sd_height,
        "sampler_name": config.sd_sampler,
    }
    
    response = requests.post(
        f"{config.sd_api_url}/sdapi/v1/txt2img",
        json=payload,
        timeout=300
    )
    
    if response.status_code == 200:
        return base64.b64decode(response.json()["images"][0])
    else:
        raise APIError(f"å›¾åƒç”Ÿæˆå¤±è´¥: {response.text}")
```

### Azure TTSé›†æˆ

```python
import azure.cognitiveservices.speech as speechsdk

def synthesize_speech(text: str, output_file: str):
    speech_config = speechsdk.SpeechConfig(
        subscription=config.azure_speech_key,
        region=config.azure_speech_region
    )
    
    speech_config.speech_synthesis_voice_name = config.azure_voice_name
    
    synthesizer = speechsdk.SpeechSynthesizer(
        speech_config=speech_config,
        audio_config=speechsdk.audio.AudioOutputConfig(filename=output_file)
    )
    
    result = synthesizer.speak_text_async(text).get()
    
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        print(f"è¯­éŸ³åˆæˆæˆåŠŸ: {output_file}")
    else:
        raise APIError(f"è¯­éŸ³åˆæˆå¤±è´¥: {result.reason}")
```

## ğŸ§ª æµ‹è¯•æ¡†æ¶

### å•å…ƒæµ‹è¯•

```python
# tests/test_config.py
import pytest
from config import Config

def test_config_loading():
    config = Config()
    assert config.llm_provider in ['openai', 'deepseek']
    assert config.project_root.exists()

def test_config_validation():
    config = Config()
    errors = config.validate_config()
    # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒæŸäº›APIå¯†é’¥å¯èƒ½æœªè®¾ç½®
    assert isinstance(errors, list)
```

### é›†æˆæµ‹è¯•

```python
# tests/test_llm_client.py
import pytest
from llm_client import llm_client

@pytest.mark.integration
def test_llm_translation():
    text = "æµ‹è¯•æ–‡æœ¬"
    result = llm_client.translate_to_english(text)
    assert isinstance(result, str)
    assert len(result) > 0
```

### Mockæµ‹è¯•

```python
# tests/test_step2.py
from unittest.mock import patch, MagicMock
import step2_txt_to_image

@patch('requests.post')
def test_image_generation(mock_post):
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"images": ["base64_image_data"]}
    mock_post.return_value = mock_response
    
    result = step2_txt_to_image.generate_image("test prompt")
    assert result is not None
    mock_post.assert_called_once()
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### å¹¶å‘å¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

def process_sentences_parallel(sentences: List[str]) -> List[str]:
    max_workers = min(len(sentences), config.max_workers_translation)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(llm_client.translate_to_english, sentence): idx 
            for idx, sentence in enumerate(sentences)
        }
        
        results = [None] * len(sentences)
        for future in tqdm(as_completed(futures), total=len(futures)):
            idx = futures[future]
            results[idx] = future.result()
            
    return results
```

### ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_translation(text: str) -> str:
    """ç¼“å­˜ç¿»è¯‘ç»“æœé¿å…é‡å¤APIè°ƒç”¨"""
    return llm_client.translate_to_english(text)

def get_cache_key(text: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(text.encode()).hexdigest()
```

### å†…å­˜ç®¡ç†

```python
import gc
from PIL import Image

def process_large_dataset(items):
    for i, item in enumerate(items):
        result = process_item(item)
        
        # å®šæœŸæ¸…ç†å†…å­˜
        if i % 100 == 0:
            gc.collect()
            
        yield result
```

## ğŸ” è°ƒè¯•æŠ€å·§

### æ—¥å¿—é…ç½®

```python
# setup_logging.py
import logging
from config import config

def setup_logging():
    level = getattr(logging, config.log_level.upper())
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('app.log'),
            logging.StreamHandler()
        ]
    )
```

### è°ƒè¯•å·¥å…·

```python
# ä½¿ç”¨pdbè°ƒè¯•
import pdb; pdb.set_trace()

# ä½¿ç”¨ipdbå¢å¼ºè°ƒè¯•
import ipdb; ipdb.set_trace()

# æ€§èƒ½åˆ†æ
import cProfile
cProfile.run('main_function()')
```

### ç¯å¢ƒå˜é‡è°ƒè¯•

```bash
# æŸ¥çœ‹å½“å‰é…ç½®
python -c "from config import config; config.print_config_summary()"

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
DEBUG=true python step1_extract_keywords-rolev1.1.py

# è¯¦ç»†æ—¥å¿—
LOG_LEVEL=DEBUG python Auto.py
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
RUN python -m spacy download zh_core_web_sm

CMD ["python", "Auto.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  txt-to-video:
    build: .
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - AZURE_SPEECH_KEY=${AZURE_SPEECH_KEY}
    volumes:
      - ./input:/app/input
      - ./output:/app/video
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒå˜é‡
export ENVIRONMENT=production
export DEBUG=false
export LOG_LEVEL=INFO

# èµ„æºé™åˆ¶
export MAX_WORKERS_TRANSLATION=8
export MAX_WORKERS_IMAGE=2

# è¶…æ—¶è®¾ç½®
export API_TIMEOUT=300
export REQUEST_RETRY_TIMES=3
```

## ğŸ“ˆ ç›‘æ§å’Œç»´æŠ¤

### å¥åº·æ£€æŸ¥

```python
# health_check.py
def check_system_health():
    checks = {
        "config": check_config(),
        "llm_service": check_llm_connection(),
        "azure_tts": check_azure_connection(),
        "stable_diffusion": check_sd_connection(),
        "disk_space": check_disk_space(),
    }
    
    all_healthy = all(checks.values())
    return {"healthy": all_healthy, "details": checks}
```

### æ€§èƒ½ç›‘æ§

```python
# metrics.py
import time
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start_time
        
        logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        return result
    return wrapper

@timing_decorator
def process_step(step_name, data):
    # å¤„ç†é€»è¾‘
    pass
```

### é”™è¯¯æŠ¥å‘Š

```python
# error_reporter.py
import traceback

def report_error(error: Exception, context: dict = None):
    error_info = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "traceback": traceback.format_exc(),
        "context": context or {},
        "timestamp": datetime.now().isoformat(),
    }
    
    logger.error(f"é”™è¯¯æŠ¥å‘Š: {error_info}")
    # å¯ä»¥å‘é€åˆ°é”™è¯¯è¿½è¸ªæœåŠ¡
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

### æäº¤ä»£ç 

1. **åˆ›å»ºåŠŸèƒ½åˆ†æ”¯**
   ```bash
   git checkout -b feature/new-feature
   ```

2. **éµå¾ªæäº¤è§„èŒƒ**
   ```bash
   git commit -m "feat: æ·»åŠ æ–°çš„LLMæä¾›å•†æ”¯æŒ"
   git commit -m "fix: ä¿®å¤å›¾åƒç”Ÿæˆè¶…æ—¶é—®é¢˜"
   git commit -m "docs: æ›´æ–°APIæ–‡æ¡£"
   ```

3. **æµ‹è¯•è¦†ç›–**
   ```bash
   pytest --cov=. --cov-report=html
   ```

4. **ä»£ç å®¡æŸ¥**
   - ç¡®ä¿ä»£ç ç¬¦åˆè§„èŒƒ
   - æ·»åŠ å¿…è¦çš„æµ‹è¯•
   - æ›´æ–°ç›¸å…³æ–‡æ¡£

### åŠŸèƒ½å¼€å‘æµç¨‹

1. **éœ€æ±‚åˆ†æ** â†’ ç¡®å®šåŠŸèƒ½èŒƒå›´å’ŒAPIè®¾è®¡
2. **è®¾è®¡æ–‡æ¡£** â†’ ç¼–å†™æŠ€æœ¯è®¾è®¡æ–‡æ¡£
3. **ç¼–ç å®ç°** â†’ éµå¾ªä»£ç è§„èŒƒå¼€å‘
4. **æµ‹è¯•éªŒè¯** â†’ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
5. **æ–‡æ¡£æ›´æ–°** â†’ æ›´æ–°ç”¨æˆ·æ–‡æ¡£å’ŒAPIæ–‡æ¡£
6. **ä»£ç å®¡æŸ¥** â†’ åŒè¡Œè¯„å®¡å’Œä¼˜åŒ–å»ºè®®

---

**éœ€è¦äº†è§£å…·ä½“ä½¿ç”¨æ–¹æ³•ï¼Œè¯·æŸ¥çœ‹ [ç”¨æˆ·æŒ‡å—](user-guide.md)ã€‚æƒ³äº†è§£é…ç½®è¯¦æƒ…ï¼Œè¯·æŸ¥çœ‹ [ç¯å¢ƒé…ç½®](environment-setup.md)ã€‚**
